{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!git clone https://github.com/KrisnaPinasthika/SparseSwin.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T17:35:00.756922Z","iopub.execute_input":"2025-07-29T17:35:00.757138Z","iopub.status.idle":"2025-07-29T17:36:01.859660Z","shell.execute_reply.started":"2025-07-29T17:35:00.757115Z","shell.execute_reply":"2025-07-29T17:36:01.858917Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'SparseSwin'...\nremote: Enumerating objects: 657, done.\u001b[K\nremote: Counting objects: 100% (61/61), done.\u001b[K\nremote: Compressing objects: 100% (49/49), done.\u001b[K\nremote: Total 657 (delta 14), reused 54 (delta 11), pack-reused 596 (from 1)\u001b[K\nReceiving objects: 100% (657/657), 1.11 GiB | 22.64 MiB/s, done.\nResolving deltas: 100% (50/50), done.\nUpdating files: 100% (548/548), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%cd SparseSwin\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T17:36:01.861451Z","iopub.execute_input":"2025-07-29T17:36:01.861665Z","iopub.status.idle":"2025-07-29T17:36:01.868011Z","shell.execute_reply.started":"2025-07-29T17:36:01.861642Z","shell.execute_reply":"2025-07-29T17:36:01.867447Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/SparseSwin\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"%ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T17:36:27.170311Z","iopub.execute_input":"2025-07-29T17:36:27.171110Z","iopub.status.idle":"2025-07-29T17:36:27.290671Z","shell.execute_reply.started":"2025-07-29T17:36:27.171066Z","shell.execute_reply":"2025-07-29T17:36:27.290007Z"}},"outputs":[{"name":"stdout","text":" additional_experiments.ipynb   \u001b[0m\u001b[01;34mModels\u001b[0m/          train_cifar_args.py\n build_model.py                 README.md        train_cifar.py\n \u001b[01;34mdatasets\u001b[0m/                      \u001b[01;34mSavedModel\u001b[0m/      train_imagenet100.py\n\u001b[01;34m'gradcam illustration'\u001b[0m/         \u001b[01;34mSources\u001b[0m/         traintest.py\n LICENSE                        \u001b[01;34mSparseSwinDet\u001b[0m/\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# %load build_model.py\nimport torch\nfrom Models.SparseSwin import SparseSwin\n\ndef buildSparseSwin(image_resolution, swin_type, num_classes, \n                    ltoken_num, ltoken_dims, num_heads, \n                    qkv_bias, lf, attn_drop_prob, lin_drop_prob, \n                    freeze_12, device):\n    \"\"\"\n    image_resolution : input image resolution (h x w x 3), input MUST be a squared image and divisible by 16\n    swin_type : Swin Transformer model type Tiny, Small, Base \n    num_classes : number of classes \n    \"\"\"\n    dims = {\n        'tiny': 96, \n        'small': 96,\n        'base': 128\n    }\n    dim_init = dims.get(swin_type.lower())\n    \n    if (dim_init == None) or ((image_resolution%16) != 0):\n        print('Check your swin type OR your image resolutions are not divisible by 16')\n        print('Remember.. it must be a squared image')\n        return None \n    \n    model = SparseSwin(\n        swin_type=swin_type, \n        num_classes=num_classes, \n        c_dim_3rd=dim_init*4, \n        hw_size_3rd=int(image_resolution/16), \n        ltoken_num=ltoken_num, \n        ltoken_dims=ltoken_dims, \n        num_heads=num_heads, \n        qkv_bias=qkv_bias, \n        lf=lf, \n        attn_drop_prob=attn_drop_prob, \n        lin_drop_prob=lin_drop_prob, \n        freeze_12=freeze_12,\n        device=device, \n    ).to(device)\n    \n    return model \n\nif __name__ == '__main__': \n    swin_type = 'tiny'\n    device = 'cuda'\n    image_resolution = 224\n    \n    model = buildSparseSwin(\n        image_resolution=image_resolution,\n        swin_type=swin_type, \n        num_classes=100, \n        ltoken_num=49, \n        ltoken_dims=512, \n        num_heads=16, \n        qkv_bias=True,\n        lf=2, \n        attn_drop_prob=.0, \n        lin_drop_prob=.0, \n        freeze_12=False,\n        device=device\n    )\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T17:45:08.020002Z","iopub.execute_input":"2025-07-29T17:45:08.020305Z","iopub.status.idle":"2025-07-29T17:45:08.025394Z","shell.execute_reply.started":"2025-07-29T17:45:08.020277Z","shell.execute_reply":"2025-07-29T17:45:08.024579Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"%%writefile train_cifar.py\nimport torch\nimport torchvision.transforms as transforms \nfrom torchvision import datasets\nimport numpy as np \nfrom traintest import train\nimport build_model as build\n\ntorch.random.manual_seed(1)\n\n# Dataset Config -------------------------------------------\nmean = np.array([0.485, 0.456, 0.406])\nstd = np.array([0.229, 0.224, 0.225])\n\ndata_transform = {\n        'train': transforms.Compose([\n                    transforms.ToTensor(),\n                    transforms.RandomResizedCrop(224),\n                    transforms.RandomHorizontalFlip(),\n                    transforms.Normalize(mean, std)\n                ]), \n        'val': transforms.Compose([\n                    transforms.ToTensor(), \n                    transforms.Resize((224, 224), antialias=None),\n                    transforms.Normalize(mean, std)\n                ])\n    }\n\nstatus = True\n# Todo: Train on CIFAR10\ntrain_dataset = datasets.CIFAR10(\n                root='./datasets/torch_cifar10/', \n                train=True, \n                transform=data_transform['train'], \n                download=status)\nval_dataset = datasets.CIFAR10(\n                root='./datasets/torch_cifar10/', \n                train=False, \n                transform=data_transform['val'], \n                download=status)\n\n# Todo: Train on CIFAR100\n# train_dataset = datasets.CIFAR100(\n#                 root='./datasets/torch_cifar100/', \n#                 train=True, \n#                 transform=data_transform['train'], \n#                 download=status)\n# val_dataset = datasets.CIFAR100(\n#                 root='./datasets/torch_cifar100/', \n#                 train=False, \n#                 transform=data_transform['val'], \n#                 download=status)\n\nbatch_size = 64\ntrain_loader = torch.utils.data.DataLoader(\n                train_dataset, \n                batch_size=batch_size, \n                shuffle=True, \n                num_workers=2, \n                pin_memory=True)\n\nval_loader = torch.utils.data.DataLoader(\n                val_dataset, \n                batch_size=batch_size, \n                shuffle=True,\n                num_workers=2, \n                pin_memory=True)\n\n\nif __name__ == '__main__':\n    dataset = 'cifar10'\n    swin_type = 'tiny'\n    reg_type, reg_lambda = 'l1', 1e-5\n    device = torch.device('cuda')\n    epochs = 100\n    show_per = 200\n    ltoken_num, ltoken_dims = 49, 256\n    lf = 2\n    \n    model = build.buildSparseSwin(\n        image_resolution=224,\n        swin_type=swin_type, \n        num_classes=10, \n        ltoken_num=ltoken_num, \n        ltoken_dims=ltoken_dims, \n        num_heads=16, \n        qkv_bias=True,\n        lf=lf, \n        attn_drop_prob=.0, \n        lin_drop_prob=.0, \n        freeze_12=False,\n        device=device)\n    \n    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n    criterion = torch.nn.CrossEntropyLoss()\n    \n    train(\n        train_loader, \n        swin_type, \n        dataset, \n        epochs, \n        model, \n        lf, \n        ltoken_num,\n        optimizer, \n        criterion, \n        device, \n        show_per=show_per,\n        reg_type=None, \n        reg_lambda=0.0, \n        validation=val_loader)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T18:04:25.100110Z","iopub.execute_input":"2025-07-29T18:04:25.100572Z","iopub.status.idle":"2025-07-29T18:04:25.106863Z","shell.execute_reply.started":"2025-07-29T18:04:25.100540Z","shell.execute_reply":"2025-07-29T18:04:25.106105Z"}},"outputs":[{"name":"stdout","text":"Overwriting train_cifar.py\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"!python train_cifar.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T18:04:27.005773Z","iopub.execute_input":"2025-07-29T18:04:27.006050Z","iopub.status.idle":"2025-07-29T20:10:09.718094Z","shell.execute_reply.started":"2025-07-29T18:04:27.006028Z","shell.execute_reply":"2025-07-29T20:10:09.717081Z"}},"outputs":[{"name":"stdout","text":"[TRAIN] Total : 782 | type : tiny | Regularization : None with lamda : 0.0\nEpoch 1/100\n  [200/782] Loss: 1.2707 Acc : 0.5519\n  [400/782] Loss: 0.9870 Acc : 0.6568\n  [600/782] Loss: 0.8680 Acc : 0.6999\n  [782/782] Loss: 0.8052 Acc : 0.7221\nLoss: 0.8052 Acc : 0.7221\n[TEST] Total : 157 | type : tiny\n[Model : tiny] Loss: 0.2206 Acc : 0.9279\n\nEpoch 2/100\n  [200/782] Loss: 0.4596 Acc : 0.8403\n  [400/782] Loss: 0.4585 Acc : 0.8407\n  [600/782] Loss: 0.4541 Acc : 0.8436\n  [782/782] Loss: 0.4531 Acc : 0.8432\nLoss: 0.4531 Acc : 0.8432\n[TEST] Total : 157 | type : tiny\n[Model : tiny] Loss: 0.1824 Acc : 0.9394\n\nEpoch 3/100\n  [200/782] Loss: 0.4120 Acc : 0.8580\n  [400/782] Loss: 0.4051 Acc : 0.8598\n  [600/782] Loss: 0.4041 Acc : 0.8600\n  [782/782] Loss: 0.3998 Acc : 0.8607\nLoss: 0.3998 Acc : 0.8607\n[TEST] Total : 157 | type : tiny\n[Model : tiny] Loss: 0.1481 Acc : 0.9487\n\nEpoch 4/100\n  [200/782] Loss: 0.3842 Acc : 0.8666\n  [400/782] Loss: 0.3713 Acc : 0.8709\n  [600/782] Loss: 0.3656 Acc : 0.8732\n  [782/782] Loss: 0.3645 Acc : 0.8737\nLoss: 0.3645 Acc : 0.8737\n[TEST] Total : 157 | type : tiny\n[Model : tiny] Loss: 0.1396 Acc : 0.9525\n\nEpoch 5/100\n  [200/782] Loss: 0.3437 Acc : 0.8809\n  [400/782] Loss: 0.3424 Acc : 0.8809\n  [600/782] Loss: 0.3433 Acc : 0.8814\n  [782/782] Loss: 0.3414 Acc : 0.8818\nLoss: 0.3414 Acc : 0.8818\n[TEST] Total : 157 | type : tiny\n[Model : tiny] Loss: 0.1257 Acc : 0.9560\n\nEpoch 6/100\n  [200/782] Loss: 0.3163 Acc : 0.8897\n  [400/782] Loss: 0.3248 Acc : 0.8875\n  [600/782] Loss: 0.3230 Acc : 0.8893\n  [782/782] Loss: 0.3259 Acc : 0.8878\nLoss: 0.3259 Acc : 0.8878\n[TEST] Total : 157 | type : tiny\n[Model : tiny] Loss: 0.1187 Acc : 0.9611\n\nEpoch 7/100\n  [200/782] Loss: 0.3057 Acc : 0.8934\n  [400/782] Loss: 0.3103 Acc : 0.8916\n  [600/782] Loss: 0.3073 Acc : 0.8923\n  [782/782] Loss: 0.3059 Acc : 0.8924\nLoss: 0.3059 Acc : 0.8924\n[TEST] Total : 157 | type : tiny\n[Model : tiny] Loss: 0.1067 Acc : 0.9648\n\nEpoch 8/100\n  [200/782] Loss: 0.3016 Acc : 0.8941\n  [400/782] Loss: 0.2989 Acc : 0.8965\n  [600/782] Loss: 0.2964 Acc : 0.8969\n  [782/782] Loss: 0.2976 Acc : 0.8969\nLoss: 0.2976 Acc : 0.8969\n[TEST] Total : 157 | type : tiny\n[Model : tiny] Loss: 0.1157 Acc : 0.9623\n\nEpoch 9/100\n  [200/782] Loss: 0.2833 Acc : 0.9019\n  [400/782] Loss: 0.2871 Acc : 0.9017\n  [600/782] Loss: 0.2861 Acc : 0.9012\n  [782/782] Loss: 0.2885 Acc : 0.9006\nLoss: 0.2885 Acc : 0.9006\n[TEST] Total : 157 | type : tiny\n[Model : tiny] Loss: 0.1173 Acc : 0.9630\n\nEpoch 10/100\n  [200/782] Loss: 0.2803 Acc : 0.9005\n  [400/782] Loss: 0.2791 Acc : 0.9006\n  [600/782] Loss: 0.2762 Acc : 0.9031\n  [782/782] Loss: 0.2751 Acc : 0.9042\nLoss: 0.2751 Acc : 0.9042\n[TEST] Total : 157 | type : tiny\n[Model : tiny] Loss: 0.1145 Acc : 0.9646\n\nEpoch 11/100\n  [200/782] Loss: 0.2626 Acc : 0.9103\n  [400/782] Loss: 0.2702 Acc : 0.9072\n  [600/782] Loss: 0.2707 Acc : 0.9067\n  [782/782] Loss: 0.2692 Acc : 0.9068\nLoss: 0.2692 Acc : 0.9068\n[TEST] Total : 157 | type : tiny\n[Model : tiny] Loss: 0.1158 Acc : 0.9650\n\nEpoch 12/100\n  [200/782] Loss: 0.2629 Acc : 0.9075\n  [400/782] Loss: 0.2600 Acc : 0.9101\n  [600/782] Loss: 0.2602 Acc : 0.9102\n  [782/782] Loss: 0.2611 Acc : 0.9097\nLoss: 0.2611 Acc : 0.9097\n[TEST] Total : 157 | type : tiny\n[Model : tiny] Loss: 0.1145 Acc : 0.9653\n\nEpoch 13/100\n  [200/782] Loss: 0.2464 Acc : 0.9155\n  [400/782] Loss: 0.2523 Acc : 0.9140\n  [600/782] Loss: 0.2507 Acc : 0.9136\n  [782/782] Loss: 0.2509 Acc : 0.9131\nLoss: 0.2509 Acc : 0.9131\n[TEST] Total : 157 | type : tiny\n[Model : tiny] Loss: 0.1122 Acc : 0.9648\n\nEpoch 14/100\n  [200/782] Loss: 0.2528 Acc : 0.9129\n  [400/782] Loss: 0.2498 Acc : 0.9150\n  [600/782] Loss: 0.2517 Acc : 0.9140\n  [782/782] Loss: 0.2516 Acc : 0.9136\nLoss: 0.2516 Acc : 0.9136\n[TEST] Total : 157 | type : tiny\n[Model : tiny] Loss: 0.1185 Acc : 0.9631\n\nEpoch 15/100\n  [200/782] Loss: 0.2374 Acc : 0.9197\n  [400/782] Loss: 0.2417 Acc : 0.9172\n  [600/782] Loss: 0.2417 Acc : 0.9163\n  [782/782] Loss: 0.2432 Acc : 0.9162\nLoss: 0.2432 Acc : 0.9162\n[TEST] Total : 157 | type : tiny\n[Model : tiny] Loss: 0.1005 Acc : 0.9710\n\nEpoch 16/100\n  [200/782] Loss: 0.2318 Acc : 0.9230\n  [400/782] Loss: 0.2342 Acc : 0.9209\n  [600/782] Loss: 0.2382 Acc : 0.9185\n  [782/782] Loss: 0.2355 Acc : 0.9199\nLoss: 0.2355 Acc : 0.9199\n[TEST] Total : 157 | type : tiny\n[Model : tiny] Loss: 0.1244 Acc : 0.9625\n\nEpoch 17/100\n  [200/782] Loss: 0.2366 Acc : 0.9166\n  [400/782] Loss: 0.2360 Acc : 0.9183\n  [600/782] Loss: 0.2340 Acc : 0.9194\n  [782/782] Loss: 0.2361 Acc : 0.9188\nLoss: 0.2361 Acc : 0.9188\n[TEST] Total : 157 | type : tiny\n[Model : tiny] Loss: 0.1139 Acc : 0.9657\n\nEpoch 18/100\n  [200/782] Loss: 0.2273 Acc : 0.9219\n  [400/782] Loss: 0.2286 Acc : 0.9211\n  [600/782] Loss: 0.2307 Acc : 0.9206\n  [782/782] Loss: 0.2325 Acc : 0.9202\nLoss: 0.2325 Acc : 0.9202\n[TEST] Total : 157 | type : tiny\n[Model : tiny] Loss: 0.1038 Acc : 0.9691\n\nEpoch 19/100\n  [200/782] Loss: 0.2289 Acc : 0.9196\n  [400/782] Loss: 0.2277 Acc : 0.9211\n  [600/782] Loss: 0.2257 Acc : 0.9222\n  [782/782] Loss: 0.2261 Acc : 0.9223\nLoss: 0.2261 Acc : 0.9223\n[TEST] Total : 157 | type : tiny\n[Model : tiny] Loss: 0.1149 Acc : 0.9666\n\nEpoch 20/100\n  [200/782] Loss: 0.2191 Acc : 0.9257\n  [400/782] Loss: 0.2161 Acc : 0.9257\n  [600/782] Loss: 0.2213 Acc : 0.9240\n  [782/782] Loss: 0.2230 Acc : 0.9229\nLoss: 0.2230 Acc : 0.9229\n[TEST] Total : 157 | type : tiny\n[Model : tiny] Loss: 0.1132 Acc : 0.9686\n\nEpoch 21/100\n  [200/782] Loss: 0.2114 Acc : 0.9287\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"%%writefile traintest.py\nimport torch\nimport os \nimport numpy as np \nimport pandas as pd\n\ndef train(train_loader, swin_type, dataset, epochs, model, lf, token_num,\n                optimizer, criterion, device, show_per,  \n                reg_type=None, reg_lambda=0., validation=None):\n    model.train()\n    total_batch = train_loader.__len__()\n    train_test_hist = []\n    best_test_acc = -99\n    \n    specific_dir = f'./SavedModel/{dataset}/SparseSwin_reg_{reg_type}_lbd_{reg_lambda}_lf_{lf}_{token_num}'\n    if f'SparseSwin_reg_{reg_type}_lbd_{reg_lambda}_lf_{lf}_{token_num}' not in os.listdir(f'./SavedModel/{dataset}/'): \n        os.mkdir(specific_dir)\n    \n    print(f\"[TRAIN] Total : {total_batch} | type : {swin_type} | Regularization : {reg_type} with lamda : {reg_lambda}\")\n    for epoch in range(epochs):\n        print(f\"Epoch {epoch+1}/{epochs}\")\n        running_loss, n_correct, n_sample = 0.0, 0.0, 0.0\n\n        for i, data in enumerate(train_loader, 0):\n            inputs, labels = data\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            # Forward pass with flexible unpacking\n            result = model(inputs)\n            if isinstance(result, tuple):\n                outputs = result[0]  # First element is the prediction tensor\n                attn_weights = result[1] if len(result) > 1 else None  # Second element is attention weights (list of tensors)\n            else:\n                outputs = result\n                attn_weights = None\n\n            \n            reg = 0\n            if reg_type == 'l1':                \n                for attn_w in attn_weights: \n                    reg += torch.sum(torch.abs(attn_w))\n                    \n            elif reg_type == 'l2':\n                for attn_w in attn_weights: \n                    reg += torch.sum(attn_w**2)\n                        \n            reg = reg_lambda * reg\n            \n            loss = criterion(outputs, labels) + reg\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n\n            with torch.no_grad(): \n                n_correct_per_batch = torch.sum(torch.argmax(outputs, dim=1) == labels)\n                n_correct += n_correct_per_batch\n                n_sample += labels.shape[0]\n                acc = n_correct / n_sample\n\n            if ((i + 1) % show_per == 0) or ((i + 1) == total_batch):\n                print(f'  [{i + 1}/{total_batch}] Loss: {(running_loss / (i + 1)):.4f} Acc : {acc:.4f}')\n\n        print(f'Loss: {(running_loss / total_batch):.4f} Acc : {(n_correct / n_sample):.4f}')\n        \n        # Save model\n        test_loss, test_acc = test(validation, swin_type=swin_type, model=model, criterion=criterion, device=device)\n        train_loss, train_acc = (running_loss / total_batch), (n_correct / n_sample)\n\n        test_loss, train_loss = round(test_loss, 4), round(train_loss, 4)\n        train_test_hist.append([train_loss, round(train_acc.item(), 4), test_loss, round(test_acc.item(), 4)])\n        \n        if test_acc >= best_test_acc:\n            best_test_acc = test_acc\n            torch.save(model.state_dict(), f'{specific_dir}/model_{epoch+1}.pt')\n    \n    train_test_hist = np.array(train_test_hist)\n    df = pd.DataFrame()\n    df['train_loss'] = train_test_hist[:, 0]\n    df['train_acc'] = train_test_hist[:, 1]\n    df['test_loss'] = train_test_hist[:, 2]\n    df['test_acc'] = train_test_hist[:, 3]\n    df.to_csv(f'{specific_dir}/hist.csv', index=None)\n    \n    # save state for last epoch\n    # torch.save({'epoch': epoch,\n    #                     'model_state_dict': model.state_dict(),\n    #                     'optimizer_state_dict': optimizer.state_dict(),\n    #                     'loss': loss}, \n    #                     f'./TrainingState/{dataset}/SparseSwin_{reg_type}_{reg_lambda}_lf_{lf}_{epoch+1}')\n    # print('Finished Training, saved training state :D')\n    # print(\"Train Loss, Train Acc, Test Loss, Test Acc\")\n    # print(train_test_hist)\n\ndef test(val_loader, swin_type, model, criterion, device):\n    model.eval()\n\n    with torch.no_grad():\n        total_batch = val_loader.__len__()\n        print(f\"[TEST] Total : {total_batch} | type : {swin_type}\")\n        running_loss, n_correct, n_sample = 0.0, 0.0, 0.0\n\n        for i, data in enumerate(val_loader, 0):\n            inputs, labels = data\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            # Forward pass with flexible unpacking\n            result = model(inputs)\n            if isinstance(result, tuple):\n                outputs = result[0]  # Only need the prediction tensor\n            else:\n                outputs = result\n\n            \n            loss = criterion(outputs, labels)\n\n            running_loss += loss.item()\n\n            n_correct_per_batch = torch.sum(torch.argmax(outputs, dim=1) == labels)\n            n_correct += n_correct_per_batch\n            n_sample += labels.shape[0]\n            acc = n_correct / n_sample\n\n    print(f'[Model : {swin_type}] Loss: {(running_loss / total_batch):.4f} Acc : {(n_correct / n_sample):.4f}')\n    print()\n    return (running_loss / total_batch), (n_correct / n_sample)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T17:59:50.772120Z","iopub.execute_input":"2025-07-29T17:59:50.772737Z","iopub.status.idle":"2025-07-29T17:59:50.779498Z","shell.execute_reply.started":"2025-07-29T17:59:50.772713Z","shell.execute_reply":"2025-07-29T17:59:50.778762Z"}},"outputs":[{"name":"stdout","text":"Overwriting traintest.py\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"%%writefile train_cifar_args.py\nimport torch\nimport torchvision.transforms as transforms \nfrom torchvision import datasets\nimport numpy as np \nfrom traintest import train\nimport build_model as build\nimport argparse\n\ntorch.random.manual_seed(1)\n\n\"\"\"\nParser: \npython train_cifar.py -dataset cifar10 -batchsize=24 -reg_type=None -sparseswin_type tiny -device cuda -epochs 1 -freeze_12 False\n\"\"\"\n\nparser = argparse.ArgumentParser()\nparser.add_argument('-dataset', help='cifar10 or cifar100', type=str, choices=['cifar10', 'cifar100'])\nparser.add_argument('-batchsize', help='the number of batch', type=int)\nparser.add_argument('-reg_type', help='the type of regularization', type=str, default='None', choices=['None', 'l1', 'l2'])\nparser.add_argument('-reg_lambda', help='the lambda for regualrization\\nIf regularization None then you dont need to specify this', type=float, default=0)\nparser.add_argument('-sparseswin_type', help='Type of the model', type=str, choices=['tiny', 'small', 'base'])\nparser.add_argument('-device', help='the computing device [cpu/cuda/etc]', type=str)\nparser.add_argument('-epochs', help='the number of epoch', type=int, default=100)\nparser.add_argument('-show_per', help='Displaying verbose per batch for each epoch', type=int, default=300)\nparser.add_argument('-lf', help='number of lf', type=int, default=2)\nparser.add_argument('-freeze_12', help='freeze? false / true', type=str, choices=['False', 'True'])\n\nargs = parser.parse_args()\nlist_of_models = {\n    'tiny': {'ltoken_num': 49, 'ltoken_dims':512},\n    'small': {'ltoken_num': 64, 'ltoken_dims':768},\n    'base': {'ltoken_num': 81, 'ltoken_dims':1024},\n}\nmodel_type = list_of_models.get(args.sparseswin_type)\n\ndataset = args.dataset.lower()\nif dataset == 'cifar10':\n    num_classes = 10 \nelse: \n    num_classes = 100\n    \nswin_type = 'tiny'\nreg_type, reg_lambda = args.reg_type, args.reg_lambda\ndevice = torch.device(args.device)\nepochs = args.epochs\nshow_per = args.show_per\nltoken_num, ltoken_dims = model_type['ltoken_num'], model_type['ltoken_dims']\nbatch_size = args.batchsize\nlf = 2\nfreeze_12 = False if args.freeze_12 == 'False' else True\n\n# Dataset Config -------------------------------------------\nmean = np.array([0.485, 0.456, 0.406])\nstd = np.array([0.229, 0.224, 0.225])\n\ndata_transform = {\n        'train': transforms.Compose([\n                    transforms.ToTensor(),\n                    transforms.RandomResizedCrop(224, antialias=None),\n                    transforms.RandomHorizontalFlip(),\n                    transforms.Normalize(mean, std)\n                ]), \n        'val': transforms.Compose([\n                    transforms.ToTensor(), \n                    transforms.Resize((224, 224), antialias=None),\n                    transforms.Normalize(mean, std)\n                ])\n    }\n\nstatus = True\nif dataset == 'cifar10':\n    # Todo: Train on CIFAR10\n    train_dataset = datasets.CIFAR10(\n                    root='./datasets/torch_cifar10/', \n                    train=True, \n                    transform=data_transform['train'], \n                    download=status)\n    val_dataset = datasets.CIFAR10(\n                    root='./datasets/torch_cifar10/', \n                    train=False, \n                    transform=data_transform['val'], \n                    download=status)\nelif dataset == 'cifar100':\n    # Todo: Train on CIFAR100\n    train_dataset = datasets.CIFAR100(\n                    root='./datasets/torch_cifar100/', \n                    train=True, \n                    transform=data_transform['train'], \n                    download=status)\n    val_dataset = datasets.CIFAR100(\n                    root='./datasets/torch_cifar100/', \n                    train=False, \n                    transform=data_transform['val'], \n                    download=status)\nelse:\n    print('Dataset is not availabel')\n\n\ntrain_loader = torch.utils.data.DataLoader(\n                train_dataset, \n                batch_size=batch_size, \n                shuffle=True, \n                num_workers=2, \n                pin_memory=True)\n\nval_loader = torch.utils.data.DataLoader(\n                val_dataset, \n                batch_size=batch_size, \n                shuffle=True,\n                num_workers=2, \n                pin_memory=True)\n\n\nif __name__ == '__main__':\n    print(f\"Training process will begin..\")\n    print(f\"SparseSwin Model : {args.sparseswin_type} | ltoken_num : {ltoken_num} | ltoken_dims : {ltoken_dims}\")\n    print(f\"dataset : {dataset}\")\n    print(f\"epochs : {epochs} | batch_size : {batch_size} | freeze12? : {freeze_12}\")\n    print(f\"device : {device}\")\n\n    model = build.buildSparseSwin(\n        image_resolution=224,\n        swin_type=swin_type, \n        num_classes=num_classes, \n        ltoken_num=ltoken_num, \n        ltoken_dims=ltoken_dims, \n        num_heads=16, \n        qkv_bias=True,\n        lf=lf, \n        attn_drop_prob=.0, \n        lin_drop_prob=.0, \n        freeze_12=freeze_12,\n        device=device)\n    \n    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n    criterion = torch.nn.CrossEntropyLoss()\n    \n    train(\n        train_loader, \n        swin_type, \n        dataset, \n        epochs, \n        model, \n        lf, \n        ltoken_num,\n        optimizer, \n        criterion, \n        device, \n        show_per=show_per,\n        reg_type=reg_type, \n        reg_lambda=reg_lambda, \n        validation=val_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T20:15:31.185160Z","iopub.execute_input":"2025-07-29T20:15:31.185807Z","iopub.status.idle":"2025-07-29T20:15:31.192593Z","shell.execute_reply.started":"2025-07-29T20:15:31.185773Z","shell.execute_reply":"2025-07-29T20:15:31.191924Z"}},"outputs":[{"name":"stdout","text":"Overwriting train_cifar_args.py\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"!python train_cifar_args.py -dataset cifar100 -epochs 100 -batchsize 100 -sparseswin_type tiny -device cuda","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T20:15:34.045304Z","iopub.execute_input":"2025-07-29T20:15:34.045992Z"}},"outputs":[{"name":"stdout","text":"100%|████████████████████████████████████████| 169M/169M [00:10<00:00, 16.0MB/s]\nTraining process will begin..\nSparseSwin Model : tiny | ltoken_num : 49 | ltoken_dims : 512\ndataset : cifar100\nepochs : 100 | batch_size : 100 | freeze12? : True\ndevice : cuda\n[TRAIN] Total : 500 | type : tiny | Regularization : None with lamda : 0\nEpoch 1/100\n  [300/500] Loss: 2.9743 Acc : 0.3103\n  [500/500] Loss: 2.5016 Acc : 0.4018\nLoss: 2.5016 Acc : 0.4018\n[TEST] Total : 100 | type : tiny\n[Model : tiny] Loss: 1.0288 Acc : 0.7187\n\nEpoch 2/100\n  [300/500] Loss: 1.4054 Acc : 0.6242\n  [500/500] Loss: 1.3747 Acc : 0.6307\nLoss: 1.3747 Acc : 0.6307\n[TEST] Total : 100 | type : tiny\n[Model : tiny] Loss: 0.7885 Acc : 0.7751\n\nEpoch 3/100\n  [300/500] Loss: 1.1991 Acc : 0.6737\n  [500/500] Loss: 1.1940 Acc : 0.6736\nLoss: 1.1940 Acc : 0.6736\n[TEST] Total : 100 | type : tiny\n[Model : tiny] Loss: 0.6973 Acc : 0.7955\n\nEpoch 4/100\n  [300/500] Loss: 1.0916 Acc : 0.7014\n  [500/500] Loss: 1.0931 Acc : 0.6996\nLoss: 1.0931 Acc : 0.6996\n[TEST] Total : 100 | type : tiny\n[Model : tiny] Loss: 0.6300 Acc : 0.8101\n\nEpoch 5/100\n  [300/500] Loss: 1.0159 Acc : 0.7185\n  [500/500] Loss: 1.0098 Acc : 0.7197\nLoss: 1.0098 Acc : 0.7197\n[TEST] Total : 100 | type : tiny\n[Model : tiny] Loss: 0.6062 Acc : 0.8153\n\nEpoch 6/100\n  [300/500] Loss: 0.9499 Acc : 0.7349\n  [500/500] Loss: 0.9584 Acc : 0.7329\nLoss: 0.9584 Acc : 0.7329\n[TEST] Total : 100 | type : tiny\n[Model : tiny] Loss: 0.5840 Acc : 0.8203\n\nEpoch 7/100\n  [300/500] Loss: 0.9016 Acc : 0.7494\n  [500/500] Loss: 0.9066 Acc : 0.7463\nLoss: 0.9066 Acc : 0.7463\n[TEST] Total : 100 | type : tiny\n[Model : tiny] Loss: 0.5678 Acc : 0.8230\n\nEpoch 8/100\n  [300/500] Loss: 0.8665 Acc : 0.7588\n  [500/500] Loss: 0.8737 Acc : 0.7565\nLoss: 0.8737 Acc : 0.7565\n[TEST] Total : 100 | type : tiny\n[Model : tiny] Loss: 0.5564 Acc : 0.8270\n\nEpoch 9/100\n  [300/500] Loss: 0.8276 Acc : 0.7698\n  [500/500] Loss: 0.8275 Acc : 0.7685\nLoss: 0.8275 Acc : 0.7685\n[TEST] Total : 100 | type : tiny\n[Model : tiny] Loss: 0.5434 Acc : 0.8331\n\nEpoch 10/100\n  [300/500] Loss: 0.7877 Acc : 0.7781\n  [500/500] Loss: 0.7951 Acc : 0.7770\nLoss: 0.7951 Acc : 0.7770\n[TEST] Total : 100 | type : tiny\n[Model : tiny] Loss: 0.5249 Acc : 0.8401\n\nEpoch 11/100\n  [300/500] Loss: 0.7745 Acc : 0.7859\n  [500/500] Loss: 0.7710 Acc : 0.7844\nLoss: 0.7710 Acc : 0.7844\n[TEST] Total : 100 | type : tiny\n[Model : tiny] Loss: 0.5210 Acc : 0.8408\n\nEpoch 12/100\n  [300/500] Loss: 0.7398 Acc : 0.7919\n  [500/500] Loss: 0.7433 Acc : 0.7906\nLoss: 0.7433 Acc : 0.7906\n[TEST] Total : 100 | type : tiny\n[Model : tiny] Loss: 0.5373 Acc : 0.8373\n\nEpoch 13/100\n  [300/500] Loss: 0.7190 Acc : 0.7987\n  [500/500] Loss: 0.7162 Acc : 0.7984\nLoss: 0.7162 Acc : 0.7984\n[TEST] Total : 100 | type : tiny\n[Model : tiny] Loss: 0.5298 Acc : 0.8394\n\nEpoch 14/100\n  [300/500] Loss: 0.7024 Acc : 0.8036\n  [500/500] Loss: 0.6997 Acc : 0.8047\nLoss: 0.6997 Acc : 0.8047\n[TEST] Total : 100 | type : tiny\n[Model : tiny] Loss: 0.5387 Acc : 0.8371\n\nEpoch 15/100\n  [300/500] Loss: 0.6765 Acc : 0.8121\n  [500/500] Loss: 0.6772 Acc : 0.8101\nLoss: 0.6772 Acc : 0.8101\n[TEST] Total : 100 | type : tiny\n[Model : tiny] Loss: 0.5249 Acc : 0.8375\n\nEpoch 16/100\n  [300/500] Loss: 0.6556 Acc : 0.8163\n  [500/500] Loss: 0.6603 Acc : 0.8144\nLoss: 0.6603 Acc : 0.8144\n[TEST] Total : 100 | type : tiny\n[Model : tiny] Loss: 0.5303 Acc : 0.8412\n\nEpoch 17/100\n  [300/500] Loss: 0.6338 Acc : 0.8236\n  [500/500] Loss: 0.6401 Acc : 0.8212\nLoss: 0.6401 Acc : 0.8212\n[TEST] Total : 100 | type : tiny\n[Model : tiny] Loss: 0.5269 Acc : 0.8457\n\nEpoch 18/100\n  [300/500] Loss: 0.6209 Acc : 0.8289\n  [500/500] Loss: 0.6277 Acc : 0.8256\nLoss: 0.6277 Acc : 0.8256\n[TEST] Total : 100 | type : tiny\n[Model : tiny] Loss: 0.5231 Acc : 0.8421\n\nEpoch 19/100\n  [300/500] Loss: 0.6143 Acc : 0.8312\n  [500/500] Loss: 0.6164 Acc : 0.8298\nLoss: 0.6164 Acc : 0.8298\n[TEST] Total : 100 | type : tiny\n[Model : tiny] Loss: 0.5469 Acc : 0.8349\n\nEpoch 20/100\n  [300/500] Loss: 0.5876 Acc : 0.8359\n  [500/500] Loss: 0.6011 Acc : 0.8330\nLoss: 0.6011 Acc : 0.8330\n[TEST] Total : 100 | type : tiny\n[Model : tiny] Loss: 0.5354 Acc : 0.8422\n\nEpoch 21/100\n  [300/500] Loss: 0.5857 Acc : 0.8355\n  [500/500] Loss: 0.5857 Acc : 0.8347\nLoss: 0.5857 Acc : 0.8347\n[TEST] Total : 100 | type : tiny\n[Model : tiny] Loss: 0.5533 Acc : 0.8373\n\nEpoch 22/100\n  [300/500] Loss: 0.5633 Acc : 0.8427\n  [500/500] Loss: 0.5742 Acc : 0.8387\nLoss: 0.5742 Acc : 0.8387\n[TEST] Total : 100 | type : tiny\n[Model : tiny] Loss: 0.5524 Acc : 0.8369\n\nEpoch 23/100\n  [300/500] Loss: 0.5546 Acc : 0.8468\n  [500/500] Loss: 0.5630 Acc : 0.8453\nLoss: 0.5630 Acc : 0.8453\n[TEST] Total : 100 | type : tiny\n[Model : tiny] Loss: 0.5403 Acc : 0.8437\n\nEpoch 24/100\n  [300/500] Loss: 0.5477 Acc : 0.8478\n  [500/500] Loss: 0.5514 Acc : 0.8466\nLoss: 0.5514 Acc : 0.8466\n[TEST] Total : 100 | type : tiny\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}